# Lecture 4: Recurrent Neural Networks (RNN)

## Overview üåê

In this session, we covered the basics of Recurrent Neural Network, space-time unfolding and Backpropagation through time (BPTT). Additionally, we explore the evolution of Recurrent Neural Networks (RNNs), delving into Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures.

## Resources üìö

- [Lecture Slides](./rnn.pdf) (pptx version: [here](./rnn.pptx))
- [train_dataset](./train_dataset.json)
- [test_dataset](./test_dataset.json)
- [dataset.ipynb](./dataset.ipynb) Script I used to build the dataset
- [ex1.ipynb](./ex1.ipynb) Solution to the first exercise
- [ex2.ipynb](./ex2.ipynb) Solution to the second exercise

## YouTube Videos üìπ:
- [AssemblyAI on RNN LSTM and GRU](https://www.youtube.com/watch?v=TLLqsEyt8NI) nice video
- [Stanford's CS230 cheatsheet on RNN](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) super in depth (not a video)
- [StatQuest on RNN](https://www.youtube.com/watch?v=AsNTP8Kwu80) very basic


## Key Topics Covered üß†

1. **Introduction to Recurrent Neural Networks (RNN):**
    - How does RNNs works
    - End loss (sequence to task) and step loss (sequence to sequence) train scenarios for RNNs
2. **Backpropagation Truth Time:**
    - Walkthrough in the BPTT algorithm. Space-time unfolding of RNNs.
3. **RNN Evolution:**
    - In-depth exploration of LSTM and GRU architectures.
    - Advantages and use cases of each architecture.

## Learning Objectives üéì

- Basic understanding of a Recurrent Neural Network
- Understand the inner workings of RNNs, including end loss and step loss training scenarios.
- Master the BPTT algorithm and the space-time unfolding of RNNs.
- Explore the advancements brought by LSTM and GRU architectures, understanding their unique features and use cases.
