# Lecture 1: Introduction to SGD and Computational Graphs

## Overview ğŸŒ

In this inaugural lecture, we embark on a foundational exploration of Stochastic Gradient Descent (SGD) and delve into the fundamentals of computational graphs. The aim is to establish a solid understanding of these essential concepts that underpin many machine learning algorithms.

## Resources ğŸ“š

- [SGD Introductory Notebook](./sgd-loss.ipynb)
- [Lecture Slides](./Autograd.pdf) (pptx version: [here](./Autograd.pptx))

## YouTube Videos ğŸ“¹:

- [Reverse mode differentiation over CG from stanford](https://www.youtube.com/watch?v=hM74RH82LyI)
- [More in depth reverse mode differentiation over CG](https://www.youtube.com/watch?v=twTIGuVhKbQ)

## Key Topics Covered ğŸ§ 

1. **Stochastic Gradient Descent (SGD):**
   - Definition and significance in optimization.
   - Intuition behind the stochastic approach and its advantages.

2. **Computational Graphs:**
   - Explanation of computational graphs as representations of mathematical expressions.
   - Distinction between forward mode and reverse mode in computational graph differentiation.
   - Practical applications and relevance in machine learning.

## Learning Objectives ğŸ“

- Grasp the core principles of SGD and comprehend its role in optimizing models during the training phase.
- Understand how computational graphs serve as a powerful tool for computing complex derivatives of mathematical expressions.
- Differentiate between forward mode and reverse mode in the context of differentiation over computational graphs.
- Be able to draw computational graphs of simple mathematical expressions and compute reverse mode and forward mode derivatives by hand.

